import ScatterChart from '@site/src/components/ScatterChart';
import { BenchmarkVisualization } from '@site/src/components/BenchmarkVisualization';
import { TaskStatisticsTable } from '@site/src/components/TaskStatisticsTable';

# Python benchmarks: comparison with other workflow engines

<div className="grid">
	<BenchmarkVisualization
		usecase="fibonacci_40_10"
		language="python"
		engines={['airflow', 'kestra', 'prefect', 'hatchet', 'temporal', 'windmill', 'windmill_dedicated']}
		workers={1}
		title="40 lightweight tasks"
		xAxisLabel="Duration (in seconds)"
	/>
</div>

<br />

We compared Airflow, Prefect, Temporal, Kestra and Windmill with the following usecases:

- One flow composed of 40 lightweight tasks.
- One flow composed of 10 long-running tasks.

:::tip More context

For additional insights about this study, refer to our [blog post](/blog/launch-week-1/fastest-workflow-engine).

:::

We chose to compute Fibonacci numbers as a simple task that can easily be run with the three orchestrators. Given that Airflow has a first class support for Python, we used Python for all 3 orchestrators. The function in charge of computing the Fibonacci numbers was very naive:

```python
def fibo(n: int):
    if n <= 1:
        return n
    else:
        return fibo(n - 1) + fibo(n - 2)
```

After some testing, we chose to compute `fibo(10)` for the lightweight tasks (taking around 10ms in our setup), and `fibo(33)` for what we called "long-running" tasks (taking at least a few hundreds milliseconds as seen in the results).

On the infrastructure side, we went simple and used the `docker-compose.yml` recommended in the documentation of each orchestrator. We deployed the orchestrators on AWS `m4-large` instances.


## Comparisons

<div className="grid">
	<BenchmarkVisualization
		usecase="fibonacci_10_33"
		language="python"
		engines={['airflow', 'kestra', 'prefect', 'temporal', 'hatchet', 'windmill', 'windmill_dedicated']}
		workers={1}
		title="10 long running tasks"
		xAxisLabel="Duration (in seconds)"
	/>
</div>

<div className="grid">
	<BenchmarkVisualization
		usecase="fibonacci_40_10"
		language="python"
		engines={['airflow', 'kestra', 'prefect', 'temporal', 'hatchet', 'windmill', 'windmill_dedicated']}
		workers={1}
		title="40 lightweight tasks"
		xAxisLabel="Duration (in seconds)"
	/>
</div>

We can exclude Airflow from the previous chart as it was performing much slower than the other orchestrators and focus on the other orchestrators:

<div className="grid">
	<BenchmarkVisualization
		usecase="fibonacci_40_10"
		language="python"
		engines={['prefect', 'kestra', 'temporal', 'hatchet', 'windmill', 'windmill_dedicated']}
		workers={1}
		title="40 lightweight tasks"
		xAxisLabel="Duration (in seconds)"
	/>
</div>


At a macro level, it took 54.668s to Airflow to execute the 10 long running tasks, where Prefect took 15.489s, Temporal 13.434s, Kestra 15.78s, Hatchet 7.793s and Windmill 8.347s in normal mode (7.701s in [dedicated worker](../../../core_concepts/25_dedicated_workers/index.mdx) mode).

The same can be observed for the 40 lightweight tasks, where Airflow took total of 116.221s, Prefect 4.872s, Temporal 2.967s, Kestra 6.04s, Hatchet 1.222s and Windmill 4.383s in normal mode (2.092s in dedicated worker mode).

By far, Airflow is the slowest. Temporal, Prefect, Kestra and Hatchet are faster, but not as fast as Windmill. For the 40 lightweight tasks, Windmill in normal mode was equivalent to Prefect and slightly slower than Temporal and Hatchet. This can be explained by the fact that the way Temporal and Hatchet works is closer to the way Windmill works in dedicated mode. I.e. Windmill in normal mode does a cold starts for each tasks, and when the tasks are numerous and lightweight, most of the execution ends up being taken by the cold start. In dedicated worker mode however, Windmill behavior is closer to Temporal and Hatchet, and we can see that the performance are similar.

But we can deep dive in a little and compare the orchestrators three categories:

- Execution time: The time it takes for the orchestrator to execute the task once is has been assigned to an executor
- Assignment time: The time is takes for a task to be assigned to an executor once it has been created in the queue
- Transition time: The time it takes for to create the following time once a task is finished

After looking at the macro numbers above, it's interesting to compare the time spent in each of the above categories, relative to the total time the orchestrator took to execute the flow.

For the 10 long running tasks flow, we see the following:

<TaskStatisticsTable
  usecase="fibonacci_10_33"
  language="python"
  engines={['airflow', 'kestra', 'prefect', 'hatchet', 'temporal', 'windmill', 'windmill_dedicated']}
  workers={1}
/>

The proportion of time spent in execution is important here since each task takes a long time to run. We see that Airflow and Prefect are spending a lot of time assigning the tasks compared to the others. When we look at the actual numbers, we see that both Prefect and Airflow are spending a lot of time assigning the first tasks, but after that, assignment duration decreases. Kestra's assignment and transition duration are somewhere in the middle, and we see that it spends most of the time in the execution phase. Airflow remains relatively slow though, and Prefect and Kestra are reaching decent performance. Temporal and Windmill in normal mode are pretty similar. Windmill in dedicated worker mode is incredibly fast at executing the jobs, at a cost of spending a little more time in the assignment phase, but overall it is the fastest.

If we look at the 40 lightweight tasks flow, we have:

<TaskStatisticsTable
  usecase="fibonacci_40_10"
  language="python"
  engines={['airflow', 'kestra', 'prefect', 'hatchet', 'temporal', 'windmill', 'windmill_dedicated']}
  workers={1}
/>

Here we see that Windmill takes a greater portion of time executing the tasks, which can be explained by the fact that Windmill runs a "cold start" for each task submitted to the worker. However, it's by far the fastest transitioning to a new task. As observed above, Windmill in dedicated worker mode is lightning fast at executing the tasks, but takes more time assigning a task to a worker.

## Conclusion

Airflow is the slowest in all categories, followed by Prefect. If you're looking for a high performance job orchestrator, they seem to not be the best option. Temporal, Kestra, Hatchet and Windmill have better performance and are closer to each other in terms of performance, but in both cases Windmill performs better either in normal mode or in [dedicated mode](../../../core_concepts/25_dedicated_workers/index.mdx). If you're looking for a job orchestrator for various long-running tasks, Windmill in normal mode will be the most performant solution, optimizing the duration of each tasks knowing that transitions and assignments will remain a small portion of the overall workload. To run lightweight tasks at a very fast pace Windmill in dedicated worker mode should be your preferred choice, provided that the tasks are similar. It is lightening fast at execution and assignment.

# Appendix: Scaling Windmill

We performed those benchmarks with a single worker assuming the capacity to process jobs would scale linearly with the number of workers deployed on the stack. We haven't verified this assumption for Airflow, Prefect, Kestra and Temporal, but we've scaled Windmill up to a 100 virtual workers to verify. And the conclusion is that it scales pretty linearly.

For this test, we've deployed the same docker compose as above on an AWS `m4.xlarge` instance (4 vCPU, 16Gb of memory) and to virtually increase the number of workers, we've used the `NUM_WORKERS` environment variable Windmill accepts. Note that it is not strictly equivalent to adding real hardware to the stack, but until we reach the maximum capacities on the instance, both in terms of CPU and memory, we can assume it's a good approximation.
The other change we had to make was to bump the `max_connections` to `1000` on Postgresql: as we're adding more and more workers, each worker needs to connect to the database and we need to increase the maximum number of connections Posgtresql allows.

The job we ran was a simple sleeping job sleeping for 100ms, which is a good average during for a job running on an orchestrator.

```python
import time
def main():
	time.sleep(0.1)
```

Finally, we've ran it on Windmill Dedicated Worker mode, and we used a specific endpoint to "bulk-create" the jobs before any worker can start pulling them from the queue. For this test to be representative, we had to measure the performance of Windmill processing a large number of jobs (10000 in this case), and we quickly realised that the time it was taking to only _insert_ the jobs one by one in the queue was non negligible and was affecting the real performance of workers.

The results are the following:

<div className="grid">
	<ScatterChart
		title="Job throughput at scale"
		xTitle="Number of workers"
		yTitle="Jobs / second"
		rawData={[
			{
				x: 2,
				y: 19.9
			},
			{
				x: 6,
				y: 59.8
			},
			{
				x: 10,
				y: 99.6
			},
			{
				x: 20,
				y: 198
			},
			{
				x: 30,
				y: 298
			},
			{
				x: 40,
				y: 361
			},
			{
				x: 50,
				y: 496
			},
			{
				x: 60,
				y: 591
			},
			{
				x: 70,
				y: 693
			},
			{
				x: 80,
				y: 786
			},
			{
				x: 90,
				y: 887
			},
			{
				x: 100,
				y: 981
			}
		]}
	/>
</div>

<details>

| **Number of workers** | **Throughput (jobs/sec) batch of 10K jobs** |
| --------------------- | ------------------------------------------- |
| 2                     | 19.9                                        |
| 6                     | 59.8                                        |
| 10                    | 99.6                                        |
| 20                    | 198                                         |
| 30                    | 298                                         |
| 40                    | 391                                         |
| 50                    | 496                                         |
| 60                    | 591                                         |
| 70                    | 693                                         |
| 80                    | 786                                         |
| 90                    | 887                                         |
| 100                   | 981                                         |

</details>

This proves that Windmill scales linearly with the number of workers (at least up to 100 workers). We can also notice that the throughput is close to the optimal: given that the job takes 100ms to be executed, N workers processing the jobs in parallel can't go above `N*100` jobs per seconds, and Windmill is pretty close.
